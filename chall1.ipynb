{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Adversarial Examples\n",
    "\n",
    "Adversarial examples, also known as evasion attacks, are intentionally-perturbed input samples aimed to mislead classification at test time. [1,2].\n",
    "\n",
    "These attacks are formulated as optimization problems that can be solved via gradient-based optimizers.\n",
    "\n",
    "Here, we will compute adversarial examples by minimizing a loss function $L$ on a target label $y_t$ (different from the true class), under manipulation constraints, as given below:\n",
    "\n",
    "$$\n",
    "\\begin{eqnarray}\n",
    "    \\mathbf x^\\star \\in {\\arg\\min}_{\\mathbf x} && L(\\mathbf x, y_t, \\theta) \\, \\\\\n",
    "    {\\rm s.t. } && \\| \\mathbf x- \\mathbf x_0\\|_2 \\leq \\varepsilon \\, , \\\\\n",
    "    && \\mathbf x_{\\rm lb} \\preceq \\mathbf x \\preceq \\mathbf x_{\\rm ub} \\, .\n",
    "\\end{eqnarray}\n",
    "$$\n",
    "\n",
    "The first constraint imposes that the adversarial perturbation will not be larger than $\\varepsilon$ (measured in $\\ell_2$ norm).\n",
    "The second constraint is a box constraint to enforce the adversarial image not to exceed the range 0-255 (or 0-1, if the input pixels are scaled).\n",
    "\n",
    "\n",
    "We solve this problem with a *projected* gradient-descent algorithm below, which iteratively projects the adversarial image on the feasible domain to ensure that the constraints remain valid.\n",
    "\n",
    "The attack is meant to manipulate the input pixels of the initial image. To this end, we will need to explicitly account for the transform/scaling performed before passing the input sample to the neural network. In particular, at each iteration, we will map the image from the pixel space onto the transformed/scaled space, update the attack point along the gradient direction in that space, project the modified image back onto the input pixel space (using an inverse-transformation function), and apply box and $\\ell_2$ constraints in the input space.\n",
    "\n",
    "\n",
    "**References**\n",
    "1.   C. Szegedy et al.,  Intriguing Properties of Neural Networks, ICLR 2014, https://arxiv.org/abs/1312.6199\n",
    "2.   B. Biggio et al., Evasion Attacks against Machine Learning at Test Time, ECML PKDD 2013, https://arxiv.org/abs/1708.06131\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/zangobot/adversarial_challenge/blob/main/chall1.ipynb)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "!npx degit https://github.com/zangobot/adversarial_challenge --force\n",
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torchvision import datasets\n",
    "\n",
    "from mnist_model import SimpleNet\n",
    "\n",
    "net = SimpleNet().load_pretrained_mnist('mnist_net.pth')\n",
    "mnist = datasets.MNIST(root='.', download=True, train=False, transform=net.get_transform())\n",
    "sample, label = mnist[350]\n",
    "sample = sample.view((1, *sample.shape))\n",
    "target_label = torch.LongTensor([2])\n",
    "\n",
    "print(f'Original label: {label}')\n",
    "iterations = 2000\n",
    "eps = 5\n",
    "loss = torch.nn.CrossEntropyLoss()\n",
    "step_size = 1\n",
    "\n",
    "x_adv = sample.clone()\n",
    "x_adv = x_adv.requires_grad_()\n",
    "\n",
    "for i in range(iterations):\n",
    "\tscores = net(x_adv)\n",
    "\n",
    "\toutput = loss(scores, target_label)\n",
    "\n",
    "\toutput.backward()\n",
    "\tgradient = x_adv.grad\n",
    "\tgradient = gradient / torch.norm(gradient, p=2)\n",
    "\tx_adv.data = x_adv.data - step_size * gradient\n",
    "\tx_adv.data = torch.clamp(x_adv, 0, 1)\n",
    "\tif torch.norm(x_adv - sample, p=2) > eps:\n",
    "\t\tdelta = x_adv.data - sample.data\n",
    "\t\tdelta = delta / torch.norm(delta, p=2)\n",
    "\t\tx_adv.data = sample.data + delta.data\n",
    "\tx_adv.grad.data.zero_()\n",
    "\n",
    "print(f'Adv loss: {output}')\n",
    "print(f'Adv label: {scores.argmax(dim=-1)}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}